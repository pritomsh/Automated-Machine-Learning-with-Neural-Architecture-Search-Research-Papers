<div align="center">
  <img src="https://github.com/pritomsh/AutoML-NeuralArchitectureSearch-Papers/blob/master/images/AutoML.png" alt="AutoML with NAS evaluation" width="600"><br>
  <strong>
    Collection of Automated Machine Learning Related Papers.
  </strong>
</div>
<br>
<p align="center">
   <a href="https://pritomsh.github.io" target="_blank"><img alt="" src="https://img.shields.io/badge/Website-EA4C89?style=normal&logo=dribbble&logoColor=white" style="vertical-align:center" /></a>
  <a href="https://github.com/pritomsh" target="_blank"><img alt="" src="https://img.shields.io/badge/GitHub-181717?style=normal&logo=github&logoColor=white" style="vertical-align:center" /></a>
  <a href="https://www.linkedin.com/in/pritomsh/" target="_blank"><img alt="" src="https://img.shields.io/badge/LinkedIn-0077B5?style=normal&logo=linkedin&logoColor=white" style="vertical-align:center" /></a>
  <a href="https://www.researchgate.net/profile/Pritom_Saha4" target="_blank"><img alt="" src="https://img.shields.io/badge/ResearchGate-00CCBB?style=normal&logo=researchgate&logoColor=white" style="vertical-align:center" /></a>
  <a href="https://www.kaggle.com/pritomsh" target="_blank"><img alt="" src="https://img.shields.io/badge/Kaggle-20BEFF?style=normal&logo=kaggle&logoColor=white" style="vertical-align:center" /></a>
</p>



## Introduction
AutoML (Automated Machine Learning) is an emerging field that aims to automate the process of building machine learning models. AutoML emerged to increase productivity and efficiency by automating as much as possible the inefficient work that occurs while repeating this process whenever machine learning is applied.


**Neural architecture search (NAS)** as it currently represents a highly popular sub-topic within the field of AutoML. NAS methods use machine learning algorithms to search through a large space of possible architectures and find the one that performs best on a given task. 

***I will attempt to design this for every aspect of NAS work.***



## Papers Table

| Research Title | Paper Link |
|----------------------------------------|--------|
|AutoML: A Survey of the State-of-the-Art|[Link](https://www.sciencedirect.com/science/article/abs/pii/S0950705120307516?via%3Dihub)|
|Neural Architecture Search with Reinforcement Learning|[Link](https://arxiv.org/abs/1611.01578)|
|Whither AutoML? Understanding the Role of Automation in Machine Learning Workflows|[Link](https://arxiv.org/abs/2101.04834)|
|Best practices for scientific research on neural architecture search|[Link](https://arxiv.org/abs/1909.02453)|
|Transfer Learning with Neural AutoML|[Link](https://arxiv.org/abs/1803.02780)|
|Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation|[Link](https://arxiv.org/abs/1901.02985)|
|DARTS: Differentiable Architecture Search|[Link](https://arxiv.org/abs/1806.09055)|
|DARTS+: Improved Differentiable Architecture Search with Early Stopping|[Link](https://arxiv.org/abs/1909.06035)|
|GLiT: Neural Architecture Search for Global and Local Image Transformer|[Link](https://arxiv.org/abs/2107.02960)|
|Two-stage architectural fine-tuning with neural architecture search using early-stopping in image classification|[Link](https://arxiv.org/abs/2202.08604)|
|Auto-GNN: Neural Architecture Search of Graph Neural Networks|[Link](https://arxiv.org/abs/1909.03184)|
|Can weight sharing outperform random architecture search? An investigation with TuNAS|[Link](https://arxiv.org/abs/2008.06120)|
|FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search|[Link](https://arxiv.org/abs/1907.01845)|
|Random Search and Reproducibility for Neural Architecture Search|[Link](https://arxiv.org/abs/1902.07638)|
|DARTS: Differentiable Architecture Search|[Link](https://arxiv.org/abs/1806.09055)|
|DARTS+: Improved Differentiable Architecture Search with Early Stopping|[Link](https://arxiv.org/abs/1909.06035)|
|Neural Network Architecture Search with Differentiable Cartesian Genetic Programming for Regression|[Link](https://arxiv.org/abs/1907.01939)|
|Automatic Design of CNNs via Differentiable Neural Architecture Search for PolSAR Image Classification|[Link](https://arxiv.org/abs/1911.06993)|
|Practical Block-wise Neural Network Architecture Generation|[Link](https://arxiv.org/abs/1708.05552)|
|MnasNet: Platform-Aware Neural Architecture Search for Mobile|[Link](https://arxiv.org/abs/1807.11626)|
|Progressive Neural Architecture Search|[Link](https://arxiv.org/abs/1712.00559)|
|Neural Architecture Search using Progressive Evolution|[Link](https://arxiv.org/abs/2203.01559)|
|Efficient Progressive Neural Architecture Search|[Link](https://arxiv.org/abs/1808.00391)|
|ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware|[Link](https://arxiv.org/abs/1812.00332)|
|Net2net: Accelerating learning via knowledge transfer|[Link](https://arxiv.org/abs/1511.05641)|
|Network Morphism|[Link](https://arxiv.org/abs/1603.01670)|
|Hierarchical Representations for Efficient Architecture Search|[Link](https://arxiv.org/abs/1711.00436)|
|Neural Architecture Search in Graph Neural Networks|[Link](https://arxiv.org/abs/2008.00077)|
|Hierarchical Neural Architecture Search for Single Image Super-Resolution|[Link](https://arxiv.org/abs/2003.04619)|
|Memory-Efficient Hierarchical Neural Architecture Search for Image Denoising|[Link](https://arxiv.org/abs/1909.08228)|
|Reinforced Evolutionary Neural Architecture Search|[Link](https://arxiv.org/abs/1808.00193)|
|On the Security Risks of AutoML|[Link](https://arxiv.org/abs/2110.06018)|
|NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search|[Link](https://arxiv.org/abs/2001.00326)|
|NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing|[Link](https://arxiv.org/abs/2006.07116)|
|Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition|[Link](https://arxiv.org/abs/2102.01063)|
|Graph HyperNetworks for Neural Architecture Search|[Link](https://arxiv.org/abs/1810.05749)|
|DSNAS: Direct Neural Architecture Search without Parameter Retraining|[Link](https://arxiv.org/abs/2002.09128)|
|Densely Connected Search Space for More Flexible Neural Architecture Search|[Link](https://arxiv.org/abs/1906.09607)|
|FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions|[Link](https://arxiv.org/abs/2004.05565)|
|A Genetic Programming Approach to Designing Convolutional Neural Network Architectures|[Link](https://arxiv.org/abs/1704.00764)|
|Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution|[Link](https://arxiv.org/abs/1804.09081)|
|BayesNAS: A Bayesian Approach for Neural Architecture Search|[Link](https://arxiv.org/abs/1905.04919)|
|Designing Neural Network Architectures using Reinforcement Learning|[Link](https://arxiv.org/abs/1611.02167)|
|Gradient Descent Effects on Differential Neural Architecture Search: A Survey|[Link](https://ieeexplore.ieee.org/abstract/document/9461192)|
|Automated machine learning on graphs: A survey|[Link](https://arxiv.org/abs/2103.00742)|
|Meta-Learning of NAS for Few-shot Learning in Medical Image Applications|[Link](https://arxiv.org/abs/2203.08951)|
|Automated Machine Learning for High-Throughput Image-Based Plant Phenotyping|[Link](https://www.mdpi.com/2072-4292/13/5/858)|
|Few-shot Neural Architecture Search|[Link](https://arxiv.org/abs/2006.06863)|
|Neural Architecture Search as Multiobjective Optimization Benchmarks: Problem Formulation and Performance Assessment|[Link](https://arxiv.org/abs/2208.04321)|
|BLOX: Macro neural architecture search benchmark and algorithms|[Link](https://arxiv.org/abs/2210.07271)|
|AutoML for Feature Selection and Model Tuning Applied to Fault Severity Diagnosis in Spur Gearboxes|[Link](https://www.mdpi.com/2297-8747/27/1/6)|
|Deep Learning Based Regression and Multi-class Models for Acute Oral Toxicity Prediction with Automatic Chemical Feature Extraction|[Link](https://arxiv.org/abs/1704.04718)|
|MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation|[Link](https://arxiv.org/abs/2003.12238)|
||[Link]()|

    
## Contributing
Welcome to the contributions research community. To contribute to this repository, please follow these guidelines:
- Fork the repository and create a new branch for your contributions.
- Ensure your research papers are properly named and organized in the appropriate folders.
- Create a pull request with a clear explanation of your contributions.
